{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c573cc8e-7800-4099-8f8c-82c5d2066752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.6794871794871795\n",
      "Best Parameters: {'nb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}\n",
      "Test Accuracy: 0.6666666666666666\n",
      "Precision: 0.4444444444444444\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         accept       0.67      1.00      0.80        22\n",
      "probably reject       0.00      0.00      0.00         1\n",
      "         reject       0.00      0.00      0.00        10\n",
      "\n",
      "       accuracy                           0.67        33\n",
      "      macro avg       0.22      0.33      0.27        33\n",
      "   weighted avg       0.44      0.67      0.53        33\n",
      "\n",
      "Some predictions:\n",
      "Review: The article presents the development of a mobile application, it does not include any element of research associated with said development. It simply focuses on the application of the RUP process for development. The texts or articles used for definitions or assertions made, for example, about RUP are not referenced \"....it is focused on \"diagrams of use cases, and risk management and management of the architecture\" as such \" If it is in circles, I assume it is a verbatim copy of some part that is not indicated. An in-depth analysis of the solution strategy is not presented. What is the logic behind the mobile application? Some problems are observed in the writing in the text, for example, \"...its objective is to ensure the production of high and higher quality software....\" The structure of the article has some deficiencies Figure 1 does not provide information to the work done, not necessary. Figure 2 is not clearly visible; it is also taken from Larman's UML book and the reference does not appear in the text. The conclusions of the work are extremely basic\n",
      "True Label: reject\n",
      "Predicted Label: accept\n",
      "\n",
      "Review: Summary: The work shows a model to measure value and seek its traceability. Co-creation of value with the client is proposed as a method of creating value. In order to analyze the contribution of co-creation, the concept of traceability is introduced. It then explains a series of variables proposed to measure the co-created value under a conceptual model together with another model that allows the traceability of the different contributions through links. Finally, the results of the model in one case are briefly presented. Evaluation: Implementing types of co-creation is very important for organizations today. It is interesting how the article mixes co-creation with traceability and proposes ways to measure it, presenting new tools for the organization. Unfortunately, the article has major weaknesses: 1) The scope may not be appropriate for a conference like INFONOR. 2) The results are brief and weakly exemplified, which makes it difficult to understand the use of the model. 3) More examples could be used for ease of reading. 4) The format of the paper has problems. Other comments: There are writing problems: a) “Organizations commit to consumers in producing initial concepts or ideas and use consumers” Abuse of the word consumer b) “the Systemic Archetypes model was validated” small error, clearly “the” is too much\n",
      "True Label: reject\n",
      "Predicted Label: accept\n",
      "\n",
      "Review: The article reflects on the current state and possible projections of formal methods in software development. Although the idea of ​​carrying out such a reflection has interesting potential, the article falls short in two key aspects: 1. There are many assertions made in the document that do not have bibliographic citations to support them, nor do they emerge from a logical deduction from from previous premises. This significantly detracts from the validity of the arguments presented. 2. All the assertions presented in the document that, as interpreted, the authors want to present as their own, are in fact widely known problems in the area of ​​formal methods. In summary, the article does not provide assertions with sufficient evidence or a solid line of argument, nor does it provide original assertions. For these reasons I consider that the article is not suitable for publication.\n",
      "True Label: reject\n",
      "Predicted Label: accept\n",
      "\n",
      "Review: - The topic of adoption of ERP systems has been developed over the last 15 years, and in particular the review of factors that imply their success is a relevant research topic. - In the work, an analysis of the ERP literature on the topic was developed that addresses critical success factors, adoption methodologies and size of the organization. - Although it is an error to indicate that \"a third type has been generated lately...\" in relation to associating project phases and FCE, since this has been done more than 12 years ago, see Quote [36] from the same work, It is not a closed topic. - I propose to the authors to define what \"ERP adoption success\" is, since there is very interesting literature related to it for their ultimate purposes (see works by Professor Markus). - Methodologically, there are three important limitations to overcome in order to advance in an impactful scientific publication. First, the impact factor of the publications has not been considered, which has rejected the development associated with this type of reviews, mixing high-impact works (for example, articles from ISI journals with an impact factor >2, such as MISQ). with works without a quality review (for example, what Scholar \"releases\" just to match the search). Second, it is said that references from 2008 onwards will be taken as a priority, but many of their references are from before that date, and that is not unusual, since between 2001 and 2004 there is an explosion of work in this specific field. Third, papers from conferences in the area of high impact in the scientific community such as AMCIS or ECIS have not been explicitly included (the first has a private database, not accessible by Google and the second is free). - The limitations stated above restrict the scope of the interesting results presented in the work.\n",
      "True Label: accept\n",
      "Predicted Label: accept\n",
      "\n",
      "Review: The article describes the concepts of data mining and the steps that a KDD process carries out in general, and the latter is associated with social networks and in particular with Facebook, presenting them as its own adaptation, which lacks foundation and logic. For example, it mentions social networks where users express tastes, feelings and other states, which are generally presented in text, and on the contrary it does not indicate anything regarding the processing of text to determine the frequency of words and thereby achieve determine the topics of conversation of a group of users on the network. It also describes some data mining techniques and mixes web mining, perhaps trying to combine different mining methods, which would be the most logical if you want to cover different aspects of social networks, that is, text content, the KDT process should be used. and not KDD, for managing associations or links, mining the web structure, etc. On page 9 he points out that he used the iconic representation technique to visualize results, however there is no evidence of this experimentation in the entire work. Finally, in the conclusions it talks about a developed guide which is not explicit anywhere in the article. It should also be noted that it has some writing errors, repetition of articles and words.\n",
      "\n",
      "True Label: reject\n",
      "Predicted Label: accept\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\vicky\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['review__text', 'preliminary_decision'], inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review__text'], df['preliminary_decision'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "nb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Use TF-IDF for feature extraction\n",
    "    ('nb', MultinomialNB())  # Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Try different n-gram ranges\n",
    "    'nb__alpha': [0.1, 1, 10]  # Smoothing parameter for Naive Bayes\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=nb_pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print some predictions\n",
    "print(\"Some predictions:\")\n",
    "for i in range(5):\n",
    "    print(\"Review:\", X_test.iloc[i])\n",
    "    print(\"True Label:\", y_test.iloc[i])\n",
    "    print(\"Predicted Label:\", y_pred[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93798a1b-6538-4d4f-b1f3-9c73aded381b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94ed76-67c3-4c84-a765-d8ca1cb25541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
